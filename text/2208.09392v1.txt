Cold Diffusion: Inverting Arbitrary Image
Transforms Without Noise

Arpit Bansal1
Eitan Borgnia∗1
Hong-Min Chu∗1
Jie S. Li1

Hamid Kazemi1
Furong Huang1
Micah Goldblum2

Jonas Geiping1
Tom Goldstein1

1University of Maryland
2New York University

Abstract

Standard diffusion models involve an image transform – adding Gaussian noise –
and an image restoration operator that inverts this degradation. We observe that the
generative behavior of diffusion models is not strongly dependent on the choice
of image degradation, and in fact an entire family of generative models can be
constructed by varying this choice. Even when using completely deterministic
degradations (e.g., blur, masking, and more), the training and test-time update
rules that underlie diffusion models can be easily generalized to create generative
models. The success of these fully deterministic models calls into question the
community’s understanding of diffusion models, which relies on noise in either
gradient Langevin dynamics or variational inference, and paves the way for gen-
eralized diffusion models that invert arbitrary processes. Our code is available at
github.com/arpitbansal297/Cold-Diffusion-Models.

Original
Forward
−−−−−−−−−−−−−−−−−−−−−−−→Degraded
Reverse
−−−−−−−−−−−−−−−−−−−−−−→Generated

Snow
Pixelate
Mask Animorph Blur
Noise

Figure 1: Demonstration of the forward and backward processes for both hot and cold diffusions.
While standard diffusions are built on Gaussian noise (top row), we show that generative models
can be built on arbitrary and even noiseless/cold image transforms, including the ImageNet-C
snowiﬁcation operator, and an animorphosis operator that adds a random animal image from AFHQ.

Preprint. Under review.

arXiv:2208.09392v1 [cs.CV] 19 Aug 2022

1
Introduction

Diffusion models have recently emerged as powerful tools for generative modeling [Ramesh et al.,
2022]. Diffusion models come in many ﬂavors, but all are built around the concept of random noise
removal; one trains an image restoration/denoising network that accepts an image contaminated with
Gaussian noise, and outputs a denoised image. At test time, the denoising network is used to convert
pure Gaussian noise into a photo-realistic image using an update rule that alternates between applying
the denoiser and adding Gaussian noise. When the right sequence of updates is applied, complex
generative behavior is observed.

The origins of diffusion models, and also our theoretical understanding of these models, are strongly
based on the role played by Gaussian noise during training and generation. Diffusion has been
understood as a random walk around the image density function using Langevin dynamics [Sohl-
Dickstein et al., 2015, Song and Ermon, 2019], which requires Gaussian noise in each step. The walk
begins in a high temperature (heavy noise) state, and slowly anneals into a “cold” state with little if
any noise. Another line of work derives the loss for the denoising network using variational inference
with a Gaussian prior [Ho et al., 2020, Song et al., 2021a, Nichol and Dhariwal, 2021].

In this work, we examine the need for Gaussian noise, or any randomness at all, for diffusion models
to work in practice. We consider generalized diffusion models that live outside the conﬁnes of the
theoretical frameworks from which diffusion models arose. Rather than limit ourselves to models
built around Gaussian noise, we consider models built around arbitrary image transformations like
blurring, downsampling, etc. We train a restoration network to invert these deformations using a
simple ℓp loss. When we apply a sequence of updates at test time that alternate between the image
restoration model and the image degradation operation, generative behavior emerges, and we obtain
photo-realistic images.

The existence of cold diffusions that require no Gaussian noise (or any randomness) during training
or testing raises questions about the limits of our theoretical understanding of diffusion models. It
also unlocks the door for potentially new types of generative models with very different properties
than conventional diffusion seen so far.

2
Background

Generative models exist for a range of modalities spanning natural language [Brown et al., 2020]
and images [Brock et al., 2019, Dhariwal and Nichol, 2021], and they can be extended to solve
important problems such as image restoration [Kawar et al., 2021a, 2022]. While GANs [Goodfellow
et al., 2014] have historically been the tool of choice for image synthesis [Brock et al., 2019, Wu
et al., 2019], diffusion models [Sohl-Dickstein et al., 2015] have recently become competitive if not
superior for some applications [Dhariwal and Nichol, 2021, Nichol et al., 2021, Ramesh et al., 2021,
Meng et al., 2021].

Both the Langevin dynamics and variational inference interpretations of diffusion models rely on
properties of the Gaussian noise used in the training and sampling pipelines. From the score-matching
generative networks perspective [Song and Ermon, 2019, Song et al., 2021b], noise in the training
process is critically thought to expand the support of the low-dimensional training distribution to
a set of full measure in ambient space. The noise is also thought to act as data augmentation to
improve score predictions in low density regions, allowing for mode mixing in the stochastic gradient
Langevin dynamics (SGLD) sampling. The gradient signal in low-density regions can be further
improved during sampling by injecting large magnitudes of noise in the early steps of SGLD and
gradually reducing this noise in later stages.

Kingma et al. [2021] propose a method to learn a noise schedule that leads to faster optimization.
Using a classic statistical result, Kadkhodaie and Simoncelli [2021] show the connection between
removing additive Gaussian noise and the gradient of the log of the noisy signal density in determin-
istic linear inverse problems. Here, we shed light on the role of noise in diffusion models through
theoretical and empirical results in applications to inverse problems and image generation.

Iterative neural models have been used for various inverse problems [Romano et al., 2016, Metzler
et al., 2017]. Recently, diffusion models have been applied to them [Song et al., 2021b] for the

2

problems of deblurring, denoising, super-resolution, and compressive sensing [Whang et al., 2021,
Kawar et al., 2021b, Saharia et al., 2021, Kadkhodaie and Simoncelli, 2021].

Although not their focus, previous works on diffusion models have included experiments with
deterministic image generation [Song et al., 2021a, Dhariwal and Nichol, 2021] and in selected
inverse problems [Kawar et al., 2022]. Here, we show deﬁnitively that noise is not a necessity in
diffusion models, and we observe the effects of removing noise for a number of inverse problems.

Despite proliﬁc work on generative models in recent years, methods to probe the properties of learned
distributions and measure how closely they approximate the real training data are by no means closed
ﬁelds of investigation. Indirect feature space similarity metrics such as Inception Score [Salimans
et al., 2016], Mode Score [Che et al., 2016], Frechet inception distance (FID) [Heusel et al., 2017],
and Kernel inception distance (KID) [Bi´nkowski et al., 2018] have been proposed and adopted to
some extent, but they have notable limitations [Barratt and Sharma, 2018]. To adopt a popular frame
of reference, we will use FID as the feature similarity metric for our experiments.

3
Generalized Diffusion

Standard diffusion models are built around two components. First, there is an image degradation
operator that contaminates images with Gaussian noise. Second, a trained restoration operator is
created to perform denoising. The image generation process alternates between the application of
these two operators. In this work, we consider the construction of generalized diffusions built around
arbitrary degradation operations. These degradations can be randomized (as in the case of standard
diffusion) or deterministic.

3.1
Model components and training

Given an image x0 ∈RN, consider the degradation of x0 by operator D with severity t, denoted
xt = D(x0, t). The output distribution D(x0, t) of the degradation should vary continuously in t,
and the operator should satisfy
D(x0, 0) = x0.

In the standard diffusion framework, D adds Gaussian noise with variance proportional to t. In our
generalized formulation, we choose D to perform various other transformations such as blurring,
masking out pixels, downsampling, and more, with severity that depends on t. We explore a range of
choices for D in Section 4.

We also require a restoration operator R that (approximately) inverts D. This operator has the
property that
R(xt, t) ≈x0.

In practice, this operator is implemented via a neural network parameterized by θ. The restoration
network is trained via the minimization problem

min
θ
Ex∼X ∥Rθ(D(x, t), t) −x∥,
(1)

where x denotes a random image sampled from distribution X and ∥· ∥denotes a norm, which we
take to be ℓ1 in our experiments. We have so far used the subscript Rθ to emphasize the dependence
of R on θ during training, but we will omit this symbol for simplicity in the discussion below.

3.2
Sampling from the model

After choosing a degradation D and training a model R to perform the restoration, these operators
can be used in tandem to invert severe degradations by using standard methods borrowed from the
diffusion literature. For small degradations (t ≈0), a single application of R can be used to obtain a
restored image in one shot. However, because R is typically trained using a simple convex loss, it
yields blurry results when used with large t. Rather, diffusion models [Song et al., 2021a, Ho et al.,
2020] perform generation by iteratively applying the denoising operator and then adding noise back
to the image, with the level of added noise decreasing over time. This corresponds to the standard
update sequence in Algorithm 1.

3

When the restoration operator is perfect, i.e.
when R(D(x0, t), t) = x0 for all t, one can
easily see that Algorithm 1 produces exact
iterates of the form xs = D(x0, s). But
what happens for imperfect restoration op-
erators? In this case, errors can cause the
iterates xs to wander away from D(x0, s),
and inaccurate reconstruction may occur.

This sampler has important mathematical properties that enable it to recover high quality results.
Speciﬁcally, for a class of linear degradation operations, it can be shown to produce exact reconstruc-
tion (i.e. xs = D(x0, s)) even when the restoration operator R fails to perfectly invert D. We discuss
this in the following section.

3.3
Properties of Algorithm 2

Figure 2: Comparison of sampling methods for cold
diffusion on the CelebA dataset. Top: Algorithm 1
produces compounding artifacts and fails to generate
a new image. Bottom: Algorithm 2 succeeds in sam-
pling a high quality image without noise.

It is clear from inspection that both Algo-
rithms 1 and 2 perfectly reconstruct the it-
erate xs = D(x0, s) for all s < t if the
restoration operator is a perfect inverse for
the degradation operator. In this section, we
analyze the stability of these algorithms to
errors in the restoration operator.

For small values of x and s, Algorithm 2 is
extremely tolerant of error in the restoration
operator R. To see why, consider a model
problem with a linear degradation function
of the form D(x, s) ≈x + s · e for some
vector e. While this ansatz may seem rather restrictive, note that the Taylor expansion of any smooth
degradation D(x, s) around x = x0, s = 0 has the form D(x, s) ≈x + s · e + HOT where HOT
denotes higher order terms. Note that the constant/zeroth-order term in this Taylor expansion is zero
because we assumed above that the degradation operator satisﬁes D(x, 0) = x.

For a degradation of the form (3.3) and any restoration operator R, the update in Algorithm 2 can be
written

xs−1 = xs −D(R(xs, s), s) + D(R(xs, s), s −1)
= D(x0, s) −D(R(xs, s), s) + D(R(xs, s), s −1)
= x0 + s · e −R(xs, s) −s · e + R(xs, s) + (s −1) · e
= x0 + (s −1) · e
= D(x0, s −1)

By induction, we see that the algorithm produces the value xs = D(x0, s) for all s < t, regardless of
the choice of R. In other words, for any choice of R, the iteration behaves the same as it would when
R is a perfect inverse for the degradation D.

By contrast, Algorithm 1 does not enjoy this behavior. In fact, when R is not a perfect inverse for D,
x0 is not even a ﬁxed point of the update rule in Algorithm 1 because x0 ̸= D(R(x, 0), 0) = R(x, 0).
If R does not perfectly invert D we should expect Algorithm 1 to incur errors, even for small values

4

of s. Meanwhile, for small values of s, the behavior of D approaches its ﬁrst-order Taylor expansion
and Algorithm 2 becomes immune to errors in R. We demonstrate the stability of Algorithm 2 vs
Algorithm 1 on a deblurring model in Figure 2.

4
Generalized Diffusions with Various Transformations

In this section, we take the ﬁrst step towards cold diffusion by reversing different degradations and
hence performing conditional generation. We will extend our methods to perform unconditional (i.e.
from scratch) generation in Section 5. We emprically evaluate generalized diffusion models trained
on different degradations with our improved sampling Algorithm 2. We perform experiments on
the vision tasks of deblurring, inpainting, super-resolution, and the unconventional task of synthetic
snow removal. We perform our experiments on MNIST [LeCun et al., 1998], CIFAR-10 [Krizhevsky,
2009], and CelebA [Liu et al., 2015]. In each of these tasks, we gradually remove the information
from the clean image, creating a sequence of images such that D(x0, t) retains less information
than D(x0, t −1). For these different tasks, we present both qualitative and quantitative results on
a held-out testing dataset and demonstrate the importance of the sampling technique described in
Algorithm 2. For all quantitative results in this section, the Frechet inception distance (FID) scores
[Heusel et al., 2017] for degraded and reconstructed images are measured with respect to the testing
data. Additional information about the quantitative results, convergence criteria, hyperparameters,
and architecture of the models presented below can be found in the appendix.

4.1
Deblurring

We consider a generalized diffusion based on a Gaussian blur operation (as opposed to Gaussian
noise) in which an image at step t has more blur than at t −1. The forward process given the Gaussian
kernels {Gs} and the image xt−1 at step t −1 can thus be written as

xt = Gt ∗xt−1 = Gt ∗. . . ∗G1 ∗x0 = ¯
Gt ∗x0 = D(x0, t),
(2)

where ∗denotes the convolution operator, which blurs an image using a kernel.

We train a deblurring model by minimizing the loss (1), and then use Algorithm 2 to invert this
blurred diffusion process for which we trained a DNN to predict the clean image ˆx0. Qualitative
results are shown in Figure 3 and quantitative results in Table 1. Qualitatively, we can see that images
created using the sampling process are sharper and in some cases completely different as compared to
the direct reconstruction of the clean image. Quantitatively we can see that the reconstruction metrics
such as RMSE and PSNR get worse when we use the sampling process, but on the other hand FID
with respect to held-out test data improves. The qualitative improvements and decrease in FID show
the beneﬁts of the generalized sampling routine, which brings the learned distribution closer to the
true data manifold.

In the case of blur operator, the sampling routine can be thought of adding frequencies at each step.
This is because the sampling routine involves the term D( ˆx0, t) −D( ˆx0, t −1) which in the case
of blur becomes ¯Gt ∗x0 −¯Gt−1 ∗x0. This results in a difference of Gaussians, which is a band
pass ﬁlter and contains frequencies that were removed at step t. Thus, in the sampling process, we
sequentially add the frequencies that were removed during the degradation process.

Degraded
Direct
Alg.
Original

Figure 3: Deblurring models trained on the MNIST, CIFAR-10, and CelebA datasets. Left to
right: degraded inputs D(x0, T) , direct reconstruction R(D(x0, T)), sampled reconstruction with
Algorithm 2, and original image.

5

Table 1: Quantitative metrics for quality of image reconstruction using deblurring models.

[['Dataset',
 'Degraded\nFID SSIM RMSE',
 'Sampled\nFID SSIM RMSE',
 'Direct\nFID SSIM RMSE']]
[['MNIST\nCIFAR-10\nCelebA',
 '438.59 0.287 0.287\n298.60 0.315 0.136\n382.81 0.254 0.193',
 '4.69 0.718 0.154\n80.08 0.773 0.075\n26.14 0.568 0.093',
 '5.10 0.757 0.142\n83.69 0.775 0.071\n36.37 0.607 0.083']]
4.2
Inpainting

We deﬁne a schedule of transforms that progressively grays-out pixels from the input image. We
remove pixels using a Gaussian mask as follows: For input images of size n × n we start with a 2D
Gaussian curve of variance β, discretized into an n × n array. We normalize so the peak of the curve
has value 1, and subtract the result from 1 so the center of the mask as value 0. We randomize the
location of the Gaussian mask for MNIST and CIFAR-10, but keep it centered for CelebA. We denote
the ﬁnal mask by zβ.

Input images x0 are iteratively masked for T steps via multiplication with a sequence of masks {zβi}
with increasing βi. We can control the amount of information removed at each step by tuning the
βi parameter. In the language of Section 3, D(x0, t) = x0 · Qt
i=1 zβi, where the operator · denotes
entry-wise multiplication.

Figure 4 presents results on test images and compares the output of the inpainting model to the
original image. The reconstructed images display reconstructed features qualitatively consistent
with the context provided by the unperturbed regions of the image. We quantitatively assess the
effectiveness of the inpainting models on each of the datasets by comparing distributional similarity
metrics before and after the reconstruction. Our results are summarized in Table 2. Note, the FID
scores here are computed with respect to the held-out validation set.

Table 2: Quantitative metrics for quality of image reconstruction using inpainting models.

[['Dataset',
 'Degraded\nFID SSIM RMSE',
 'Sampled\nFID SSIM RMSE',
 'Direct\nFID SSIM RMSE']]
[['MNIST\nCIFAR-10\nCelebA',
 '108.48 0.490 0.262\n40.83 0.615 0.143\n127.85 0.663 0.155',
 '1.61 0.941 0.068\n8.92 0.859 0.068\n5.73 0.917 0.043',
 '2.24 0.948 0.060\n9.97 0.869 0.063\n7.74 0.922 0.039']]
4.3
Super-Resolution

For this task, the degradation operator downsamples the image by a factor of two in each direction.
This takes place, once for each values of t, until a ﬁnal resolution is reached, 4×4 in the case of
MNIST and CIFAR-10 and 2×2 in the case of Celeb-A. After each down-sampling, the lower-
resolution image is resized to the original image size, using nearest-neighbor interpolation. Figure 5
presents example testing data inputs for all datasets and compares the output of the super-resolution
model to the original image. Though the reconstructed images are not perfect for the more challenging

Degraded
Direct
Alg.
Original

Figure 4: Inpainting models trained on the MNIST, CIFAR-10, and CelebA datasets. Left to
right: Degraded inputs D(x0, T) , direct reconstruction R(D(x0, T)), sampled reconstruction with
Algorithm 2, and original image.

6

datasets, the reconstructed features are qualitatively consistent with the context provided by the low
resolution image.

Degraded
Direct
Alg.
Original

Figure 5: Superresolution models trained on the MNIST, CIFAR-10, and CelebA datasets. Left to
right: degraded inputs D(x0, T) , direct reconstruction R(D(x0, T)), sampled reconstruction with
Algorithm 2, and original image.

Table 3 compares the distributional similarity metrics between degraded/reconstructed images and
test samples.

Table 3: Quantitative metrics for quality of image reconstruction using super-resolution models.

[['Dataset',
 'Degraded\nFID SSIM RMSE',
 'Sampled\nFID SSIM RMSE',
 'Direct\nFID SSIM RMSE']]
[['MNIST\nCIFAR-10\nCelebA',
 '368.56 0.178 0.231\n358.99 0.279 0.146\n349.85 0.335 0.225',
 '4.33 0.820 0.115\n152.76 0.411 0.155\n96.92 0.381 0.201',
 '4.05 0.823 0.114\n169.94 0.420 0.152\n112.84 0.400 0.196']]
4.4
Snowiﬁcation

Apart from traditional degradations, we additionally provide results for the task of synthetic snow
removal using the ofﬁcal implementation of the snowiﬁcation transform from ImageNet-C [Hendrycks
and Dietterich, 2019]. The purpose of this experiment is to demonstrate that generalized diffusion
can succeed even with exotic transforms that lack the scale-space and compositional properties of
blur operators. Similar to other tasks, we degrade the images by adding snow, such that the level of
snow increases with step t. We provide more implementation details in Appendix.

We illustrate our desnowiﬁcation results in Figure 6. We present testing examples, as well as their
snowiﬁed images, from all the datasets, and compare the desnowiﬁed results with the original images.
The desnowiﬁed images feature near-perfect reconstruction results for CIFAR-10 examples with
lighter snow, and exhibit visually distinctive restoration for Celeb-A examples with heavy snow. We
provide quantitative results in Table 4.

Degraded
Direct
Alg.
Original

Figure 6: Desnowiﬁcation models trained on the CIFAR-10, and CelebA datasets. Left to right: de-
graded inputs D(x0, T) , direct reconstruction R(D(x0, T)), sampled reconstruction with Algorithm
2, and original image.

5
Cold Generation

Diffusion models can successfully learn the underlying distribution of training data, and thus generate
diverse, high quality images [Song et al., 2021a, Dhariwal and Nichol, 2021, Jolicoeur-Martineau
et al., 2021, Ho et al., 2022]. We will ﬁrst discuss deterministic generation using Gaussian noise

7

Table 4: Quantitative metrics for quality of image reconstruction using desnowiﬁcation models.

[['Dataset', 'Degraded Image\nFID SSIM RMSE', 'Reconstruction\nFID SSIM RMSE']]
[['CIFAR-10\nCelebA',
 '125.63 0.419 0.327\n398.31 0.338 0.283',
 '31.10 0.074 0.838\n27.09 0.033 0.907']]
and then discuss in detail unconditional generation using deblurring. Finally, we provide a proof of
concept that the Algorithm 2 can be extended to other degradations.

5.1
Generation using deterministic noise degradation

as the (deterministic) interpolation between data point x and a ﬁxed noise pattern z ∈N(0, 1), for
increasing αt < αt−1, ∀1 ≤t ≤T as in Song et al. [2021a]. Algorithm 2 can be applied in this case
by ﬁxing the noise z used in the degradation operator D(x, s). Alternatively, one can deterministically
calculate the noise vector z to be used in step t of reconstruction by using the formula

The second method turns out to be closely related to the deterministic sampling proposed in Song
et al. [2021a], with some differences in the formulation of the training objective. We discuss this
relationship in detail in Appendix A.6. We present quantitative results for CelebA and AFHQ datasets
using the ﬁxed noise method and the estimated noise method (using ˆz) in Table 5.

5.2
Image generation using blur

The forward diffusion process in noise-based diffusion models has the advantage that the degraded
image distribution at the ﬁnal step T is simply an isotropic Gaussian. One can therefore perform
(unconditional) generation by ﬁrst drawing a sample from the isotropic Gaussian, and sequentially
denoising it with backward diffusion.

When using blur as a degradation, the fully degraded images do not form a nice closed-form
distribution that we can sample from. They do, however, form a simple enough distribution that can
be modeled with simple methods. Note that every image x0 degenerates to an xT that is constant (i.e.,
every pixel is the same color) for large T. Furthermore, the constant value is exactly the channel-wise
mean of the RGB image x0, and can be represented with a 3-vector. This 3-dimensional distribution
is easily represented using a Gaussian mixture model (GMM). This GMM can be sampled to produce
the random pixel values of a severely blurred image, which can be deblurred using cold diffusion to
create a new image.

Our generative model uses a blurring schedule where we progressively blur each image with a
Gaussian kernel of size 27x27 over 300 steps. The standard deviation of the kernel starts at 1 and
increases exponentially at the rate of 0.01. We then ﬁt a simple GMM with one component to the
distribution of channel-wise means. To generate an image from scratch, we sample the channel-wise
mean from the GMM, expand the 3D vector into a 128 × 128 image with three channels, and then
apply Algorithm 2.

Empirically, the presented pipeline generates images with high ﬁdelity but low diversity, as reﬂected
quantitatively by comparing the perfect symmetry column with results from hot diffusion in Table 5.
We attribute this to the perfect correlation between pixels of xT sampled from the channel-wise
mean Gaussian mixture model. To break the symmetry between pixels, we add a small amount of
Gaussian noise (of standard deviation 0.002) to each sampled xT . As shown in Table 5, the simple
trick drastically improves the quality of generated images. We also present the qualitative results for
cold diffusion using blur transformation in Figure 7, and further discuss the necessity of Algorithm 2

8

for generation in Appendix A.7.

Table 5: FID scores for CelebA and AFHQ datasets using hot (using noise) and cold diffusion (using
blur transformation). This table shows that This table also shows that breaking the symmetry withing
pixels of the same channel further improves the FID scores.

[['Dataset',
 'Hot Diffusion Cold Diffusion\n'
 'Fixed Noise Estimated Noise Perfect symmetry Broken symmetry']]
[['CelebA\nAFHQ', '59.91 23.11\n25.62 20.59', '97.00 49.45\n93.05 54.68']]
Figure 7: Examples of generated samples from 128 × 128 CelebA and AFHQ datasets using cold
diffusion with blur transformation

5.3
Generation using other transformations

In this section, we further provide a proof of concept that generation can be extended to other
transformations. Speciﬁcally, we show preliminary results on inpainting, super-resolution, and
animorphosis. Inspired by the simplicity of the degraded image distribution for the blurring routine
presented in the previous section, we use degradation routines with predictable ﬁnal distributions
here as well.

To use the Gaussian mask transformation for generation, we modify the masking routine so the
ﬁnal degraded image is completely devoid of information. One might think a natural option is to
send all of the images to a completely black image xT , but this would not allow for any diversity
in generation. To get around this maximally non-injective property, we instead make the mask turn
all pixels to a random, solid color. This still removes all of the information from the image, but it
allows us to recover different samples from the learned distribution via Algorithm 2 by starting off
with different color images. More formally, a Gaussian mask Gt = Qt
i=1 zβi is created in a similar
way as discussed in the Section 4.2, but instead of multiplying it directly to the image x0, we create
xt as follows:
xt = Gt ∗x0 + (1 −Gt) ∗c

where c is an image of a randomly sampled color.

For super-resolution, the routine down-samples to a resolution of 2 × 2, or 4 values in each channel.
These degraded images can be represented as one-dimensional vectors, and their distribution is
modeled using one Gaussian distribution. Using the same methods described for generation using

9

blurring described above, we sample from this Gaussian-ﬁtted distribution of the lower-dimensional
degraded image space and pass this sampled point through the generation process trained on super-
resolution data to create one output.

Additionally to show one can invert nearly any transformation, we include a new transformation
deemed animorphosis, where we iteratively transform a human face from CelebA to an animal
face from AFHQ. Though we chose CelebA and AFHQ for our experimentation, in principle such
interpolation can be done for any two initial data distributions.

Note this is essentially the same as the noising procedure, but instead of adding noise we are adding a
progressively higher weighted AFHQ image. In order to sample from the learned distribution, we
sample a random image of an animal and use Algorithm 2 to reverse the animorphosis transformation.

We present results for the CelebA dataset, and hence the quantitative results in terms of FID scores
for inpainting, super-resolution and animorphosis are 90.14, 92.91 and 48.51 respectively. We further
show some qualitative samples in Figure 8, and in Figure 1.

Figure 8: Preliminary demonstration of the generative abilities of other cold diffusins on the 128×128
CelebA dataset. The top row is with animorphosis models, the middle row is with inpainting models,
and the bottom row exhibits super-resolution models.

6
Conclusion

Existing diffusion models rely on Gaussian noise for both forward and reverse processes. In this
work, we ﬁnd that the random noise can be removed entirely from the diffusion model framework,
and replaced with arbitrary transforms. In doing so, our generalization of diffusion models and
their sampling procedures allows us to restore images afﬂicted by deterministic degradations such as
blur, inpainting and downsampling. This framework paves the way for a more diverse landscape of
diffusion models beyond the Gaussian noise paradigm. The different properties of these diffusions
may prove useful for a range of applications, including image generation and beyond.

References

Shane Barratt and Rishi Sharma. A note on the inception score. arXiv preprint arXiv:1801.01973,
2018.

Mikołaj Bi´nkowski, Danica J Sutherland, Michael Arbel, and Arthur Gretton. Demystifying mmd
gans. arXiv preprint arXiv:1801.01401, 2018.

Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high ﬁdelity
natural image synthesis. 2019.

10

Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel
Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler,
Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott
Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya
Sutskever, and Dario Amodei. Language models are few-shot learners. Advances in Neural
Information Processing Systems, 33, 2020.

Tong Che, Yanran Li, Athul Paul Jacob, Yoshua Bengio, and Wenjie Li. Mode regularized generative
adversarial networks. arXiv preprint arXiv:1612.02136, 2016.

Prafulla Dhariwal and Alexander Quinn Nichol. Diffusion models beat gans on image synthesis.
volume 34, 2021.

Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron C. Courville, and Yoshua Bengio.
Generative adversarial nets.
Advances in Neural
Information Processing Systems, 27, 2014.

Dan Hendrycks and Thomas G. Dietterich. Benchmarking neural network robustness to common
corruptions and perturbations. In International Conference on Learning Representations, ICLR
2019. OpenReview.net, 2019.

Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans
trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural
information processing systems, 30, 2017.

Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in
Neural Information Processing Systems, 32, 2020.

Jonathan Ho, Chitwan Saharia, William Chan, David J. Fleet, Mohammad Norouzi, and Tim Salimans.
Cascaded diffusion models for high ﬁdelity image generation. J. Mach. Learn. Res., 23, 2022.

Alexia Jolicoeur-Martineau, Rémi Piché-Taillefer, Ioannis Mitliagkas, and Remi Tachet des Combes.
Adversarial score matching and improved sampling for image generation. International Conference
on Learning Representations, 2021.

Zahra Kadkhodaie and Eero Simoncelli. Stochastic solutions for linear inverse problems using the
prior implicit in a denoiser. Advances in Neural Information Processing Systems, 34, 2021.

Bahjat Kawar, Gregory Vaksman, and Michael Elad.
SNIPS: solving noisy inverse problems
stochastically. volume 34, 2021a.

Bahjat Kawar, Gregory Vaksman, and Michael Elad. Stochastic image denoising by sampling from
the posterior distribution. International Conference on Computer Vision Workshops, 2021b.

Bahjat Kawar, Michael Elad, Stefano Ermon, and Jiaming Song. Denoising diffusion restoration
models. arXiv preprint arXiv:2201.11793, 2022.

Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.

Diederik P. Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. On density estimation with diffusion
models. Advances in Neural Information Processing Systems, 34, 2021.

Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.

Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proc. IEEE, 86(11):2278–2324, 1998.

Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In
Proceedings of International Conference on Computer Vision (ICCV), December 2015.

Chenlin Meng, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Image
synthesis and editing with stochastic differential equations. arXiv preprint arXiv:2108.01073,
2021.

11

Christopher A. Metzler, Ali Mousavi, and Richard G. Baraniuk. Learned D-AMP: principled neural
network based compressive image recovery. Advances in Neural Information Processing Systems,
30, 2017.

Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew,
Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with
text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021.

Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic mod-
els. In Proceedings of the 38th International Conference on Machine Learning, volume 139 of
Proceedings of Machine Learning Research, pages 8162–8171, 2021.

Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen,
and Ilya Sutskever. Zero-shot text-to-image generation. International Conference on Machine
Learning, 2021.

Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-
conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.

Yaniv Romano, Michael Elad, and Peyman Milanfar. The little engine that could: Regularization by
denoising (RED). arXiv preprint arXiv:1611.02862, 2016.

Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David J. Fleet, and Mohammad Norouzi.
Image super-resolution via iterative reﬁnement. arXiv preprint arXiv:2104.07636, 2021.

Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.
Improved techniques for training gans. Advances in neural information processing systems, 29,
2016.

Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised
learning using nonequilibrium thermodynamics. In International Conference on Machine Learning,
volume 37 of JMLR Workshop and Conference Proceedings, 2015.

Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. International
Conference on Learning Representations, 2021a.

Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution.
Advances in Neural Information Processing Systems, 32, 2019.

Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben
Poole. Score-based generative modeling through stochastic differential equations. International
Conference on Learning Representations, 2021b.

Jay Whang, Mauricio Delbracio, Hossein Talebi, Chitwan Saharia, Alexandros G. Dimakis, and
Peyman Milanfar. Deblurring via stochastic reﬁnement. arXiv preprint arXiv:2112.02475, 2021.

Yan Wu, Jeff Donahue, David Balduzzi, Karen Simonyan, and Timothy P. Lillicrap. LOGAN: latent
optimisation for generative adversarial networks. arXiv preprint arXiv:1912.00953, 2019.

12

A
Appendix

A.1
Deblurring

For the deblurring experiments, we train the models on different datasets for 700,000 gradient steps.
We use the Adam [Kingma and Ba, 2014] optimizer with learning rate 2 × 10−5. The training was
done on the batch size of 32, and we accumulate the gradients every 2 steps. Our ﬁnal model is an
Exponential Moving Average of the trained model with decay rate 0.995 which is updated after every
10 gradient steps.
For the MNIST dataset, we blur recursively 40 times, with a discrete Gaussian kernel of size 11x11
and a standard deviation 7. In the case of CIFAR-10, we recursively blur with a Gaussian kernel
of ﬁxed size 11x11, but at each step t, the standard deviation of the Gaussian kernel is given by
0.01 ∗t + 0.35. The blur routine for CelebA dataset involves blurring images with a Gaussian kernel
of 15x15 and the standard deviation of the Gaussian kernel grows exponentially with time t at the
rate of 0.01.

Figure 9 shows an additional nine images for each of MNIST, CIFAR-10 and CelebA. Figures 19 and
20 show the iterative sampling process using a deblurring model for ten example images from each
dataset. We further show 400 random images to demonstrate the qualitative results in the Figure 21.

Degraded
Direct
Alg.
Original

Figure 9: Additional examples from deblurring models trained on the MNIST, CIFAR-10, and CelebA
datasets. Left to right: degraded inputs D(x0, T) , direct reconstruction R(D(x0, T)), sampled
reconstruction with Algorithm 2, and original image.

A.2
Inpainting

For the inpainting transformation, models were trained on different datasets with 60,000 gradient
steps. The models were trained using Adam [Kingma and Ba, 2014] optimizer with learning rate
2×10−5. We use batch size 64, and the gradients are accumulated after every 2 steps. The ﬁnal model
is an Exponential Moving Average of the trained model with decay rate 0.995. This EMA model
is updated after every 10 gradient steps. For all our inpainting experiments we use a randomized
Gaussian mask and T = 50 with β1 = 1 and βi+1 = βi + 0.1.

To avoid potential leakage of information due to ﬂoating point computation of the Gaussian mask,
we discretize the masked image before passing it through the inpainting model. This was done by
rounding all pixel values to the eight most signiﬁcant digits.

13

Figure 11 shows nine additional inpainting examples on each of the MNIST, CIFAR-10, and CelebA
datasets. Figure 10 demonstrates an example of the iterative sampling process of an inpainting model
for one image in each dataset.

A.3
Super-Resolution

We train the super-resolution model per Section 3.1 for 700,000 iterations. We use the Adam [Kingma
and Ba, 2014] optimizer with learning rate 2 × 10−5. The batch size is 32, and we accumulate the
gradients every 2 steps. Our ﬁnal model is an Exponential Moving Average of the trained model with
decay rate 0.995. We update the EMA model every 10 gradient steps.

The number of time-steps depends on the size of the input image and the ﬁnal image. For MNIST and
for CIFAR10, the number of time steps is 3, as it takes three steps of halving the resolution to reduce
the initial image down to 4 × 4. For CelebA, the number of time steps is 6 to reduce the initial image
down to 2 × 2. For CIFAR10, we apply random crop and random horizontal ﬂip for regularization.

Figure 13 shows an additional nine super-resolution examples on each of the MNIST, CIFAR-10, and
CelebA datasets. Figure 12 shows one example of the progressive increase in resolution achieved
with the sampling process using a super-resolution model for each dataset.

A.4
Colorization

Here we provide results for the additional task of colorization. Starting with the original RGB-
image x0, we realize colorization by iteratively desaturating for T steps until the ﬁnal image xT
is a fully gray-scale image. We use a series of three-channel 1 × 1 convolution ﬁlters z(α) =
{z1(α), z2(α), z3(α)} with the form

and obtain D(x, t) = z(αt) ∗x via a schedule deﬁned as α1, . . . , αt for each respective step. Notice
that a gray image is obtained when xT = z(1) ∗x0.

We can tune the ratio αt to control the amount of information removed in each step. For our
experiment, we schedule the ratio such that for every t we have

This schedule ensures that color information lost between steps is smaller in earlier stage of the
diffusion and becomes larger as t increases.

We train the models on different datasets for 700,000 gradient steps. We use Adam [Kingma and Ba,
2014] optimizer with learning rate 2 × 10−5. We use batch size 32, and we accumulate the gradients
every 2 steps. Our ﬁnal model is an exponential moving average of the trained model with decay rate
0.995. We update the EMA model every 10 gradient steps. For CIFAR-10 we use T = 50 and for
CelebA we use T = 20.

Figure 10: Progressive inpainting of selected masked MNIST, CIFAR-10, and CelebA images.

14

Degraded
Direct
Alg.
Original

Figure 11: Additional examples from inpainting models trained on the MNIST, CIFAR-10, and
CelebA datasets. Left to right: degraded inputs D(x0, T) , direct reconstruction R(D(x0, T)),
sampled reconstruction with Algorithm 2, and original image.

Figure 12: Progressive upsampling of selected downsampled MNIST, CIFAR-10, and CelebA images.
The original image is at the left for each of these progressive upsamplings.

We illustrate our recolorization results in Figure 14. We present testing examples, as well as their
grey scale images, from all the datasets, and compare the recolorization results with the original
images. The recolored images feature correct color separation between different regions, and feature
various and yet semantically correct colorization of objects. Our sampling technique still yields
minor differences in comparison to the direct reconstruction, although the change is not visually
apparent. We attribute this to the shape restriction of colorization task, as human perception is rather
insensitive to minor color change. We also provide quantitative measurement for the effectiveness
of our recolorization results in terms of different similarity metrics, and summarize the results in
Table 6.

Table 6: Quantitative metrics for quality of image reconstruction using recolorization models for all
three channel datasets.

[['Dataset', 'Degraded Image\nFID SSIM RMSE', 'Reconstruction\nFID SSIM RMSE']]
[['CIFAR-10\nCelebA',
 '97.39 0.937 0.078\n41.20 0.942 0.089',
 '45.74 0.942 0.069\n17.50 0.973 0.042']]
15

Degraded
Direct
Alg.
Original

Figure 13: Additional examples from super-resolution models trained on the MNIST, CIFAR-10,
and CelebA datasets. Left to right: degraded inputs D(x0, T) , direct reconstruction R(D(x0, T)),
sampled reconstruction with Algorithm 2, and original image.

Degraded
Direct
Alg.
Original

Figure 14: Recolorization models trained on the CIFAR-10 and CelebA datasets. Left to right: de-
graded inputs D(x0, T) , direct reconstruction R(D(x0, T)), sampled reconstruction with Algorithm
2, and original image.

A.5
Image Snow

Here we provide results for the additional task of snowiﬁcation, which is a direct adaptation of
the ofﬁcal implementation of ImageNet-C snowiﬁcation process [Hendrycks and Dietterich, 2019].
To determine the snow pattern of a given image x0 ∈RC×H×W , we ﬁrst construct a seed matrix
SA ∈RH×W where each entry is sampled from a Gaussian distribution N(µ, σ). The upper-left
corner of SA is then zoomed into another matrix SB ∈RH×W with spline interpolation. Next, we
create a new matrix SC by ﬁltering each value of SB with a given threshold c1 as

SC[i][j] =
0,
SB[i][j] ≤c1
SB[i][j],
SB[i][j] > c1

and clip each entry of SC into the range [0, 1]. We then convolve SC using a motion blur kernel with
standard deviation c2 to create the snow pattern S and its up-side-down rotation S′. The direction of
the motional blur kernel is randomly chosen as either vertical or horizontal. The ﬁnal snow image is

16

created by again clipping each value of x0 + S + S′ into the range [0, 1]. For simplicity, we abstract
the process as a function h(x0, SA, c0, c1).

Degraded
Direct
Alg.
Original

Figure 15: Additional examples from Desnowiﬁcation models trained on the CIFAR-10 and CelebA
datasets. Left to right: degraded inputs D(x0, T) , direct reconstruction R(D(x0, T)), sampled
reconstruction with Algorithm 2, and original image.

To create a series of T images with increasing snowiﬁcation, we linearly interpolate c0 and c1 between
[cstart
0
, cend
0 ] and [cstart
1
, cend
1 ] respectively, to create c0(t) and c1(t), t = 1, . . . , T. Then for each x0, a
seed matrix Sx is sampled, the motion blur direction is randomized, and we construct each related
xt by xt = h(x0, Sx, c0(t), c1(t)). Visually, c0(t) dictates the severity of the snow, while c1(t)
determines how “windy" the snowiﬁed image seems.

For both CIFAR-10 and Celeb-A, we use the same Gaussian distribution with parameters µ = 0.55
and σ = 0.3 to generate the seed matrix. For CIFAR-10, we choose cstart
0
= 1.15, cend
0
= 0.7,
cstart
1
= 0.05 and cend
1
= 16, which generates a visually lighter snow. For Celeb-A, we choose
cstart
0
= 1.15, cend
0
= 0.55, cstart
1
= 0.05 and cend
1
= 20, which generates a visually heavier snow.

We train the models on different datasets for 700,000 gradient steps. We use Adam [Kingma and Ba,
2014] optimizer with learning rate 2 × 10−5. We use batch size 32, and we accumulate the gradients
every 2 steps. Our ﬁnal model is an exponential moving average of the trained model with decay
rate 0.995. We update the EMA model every 10 gradient steps. For CIFAR-10 we use T = 200 and
for CelebA we use T = 200. We note that the seed matrix is resampled for each individual training
batch, and hence the snow pattern varies across the training stage.

A.6
Generation using noise : Further Details

Here we will discuss in further detail on the similarity between the sampling method proposed in
Algorithm 2 and the deterministic sampling in DDIM [Song et al., 2021a]. Given the image xt at
step t, we have the restored clean image ˆx0 from the diffusion model. Hence given the estimated ˆx0
and xt, we can estimate the noise z(xt, t) (or ˆz) as

Thus, the D( ˆx0, t) and D( ˆx0, t −1) can be written as

using which the sampling process in Algorithm 2 to estimate xt−1 can be written as,

17

xt−1 = xt −D( ˆx0, t) + D( ˆx0, t −1)

which is same as the sampling method as described in [Song et al., 2021a].

A.7
Generation using blur transformation: Further Details

Figure 16: Examples of generated samples from 128×128 CelebA and AFHQ datasets using Method
2 with perfect symmetry.

The Figure 16, shows the generation without breaking any symmetry within each channel are quite
promising as well.

Necessity of Algorithm 2: In the case of unconditional generation, we observe a marked superiority
in quality of the sampled reconstruction using Algorithm 2 over any other method considered. For
example, in the broken symmetry case, the FID of the directly reconstructed images is 257.69 for
CelebA and 214.24 for AFHQ, which are far worse than the scores of 49.45 and 54.68 from Table 5.
In Figure 17, we also give a qualitative comparison of this difference. We can also clearly see from
Figure 18 that Algorithm 1, the method used in Song et al. [2021b] and Ho et al. [2020], completely
fails to produce an image close to the target data distribution.

18

Figure 17: Comparison of direct reconstruction with sampling using Algorithm 2 for generation with
Method 2 and broken symmetry. Left-hand column is the initial cold images generated using the
simple Gaussian model. Middle column has images generated in one step (i.e. direct reconstruction).
Right-hand column are the images sampled with Algorithm 2. We present results for both CelebA
(top) and AFHQ (bottom) with resolution 128 × 128.

Figure 18: Comparison of Algorithm 1 (top row) and Algorithm 2 (bottom row) for generation with
Method 2 and broken symmetry on 128 × 128 CelebA dataset. We demonstrate that Algorithm 1
fails completely to generate a new image.

19

Figure 19: Progressive deblurring of selected blurred MNIST and CIFAR-10 images.

20

Figure 20: Progressive deblurring of selected blurred CelebA images.

21

Figure 21: Deblurred Cifar10 images

22